{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Colab to a Chameleon server\n",
    "\n",
    "This notebook describes how to connect Colab to a server running on Chameleon. This allows you to run experiments requiring bare metal access, storage, memory, GPU and compute that exceeds the abilities of Colab's hosted runtime, but with Colab's familiar interface (and notebooks stored in your Google Drive). It also allows you to easily go back and forth between the convenience of Colab's hosted runtime and Chameleon's greater capabilities, depending on the needs of your experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provision the resource\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check resource availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will try to reserve a bare metal Ubuntu server with RTX6000 GPU on CHI@UC - pending availability. Before you begin, you should check the host calendar at [https://chi.uc.chameleoncloud.org/project/leases/calendar/host/](https://chi.uc.chameleoncloud.org/project/leases/calendar/host/). In the \"Node Type\" dropdown, filter on `gpu_rtx_6000` and make sure some hosts are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chameleon configuration\n",
    "\n",
    "You can change your Chameleon project name (if not using the one that is automatically configured in the JupyterHub environment) and the site on which to reserve resources (depending on availability) in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chi, os, time, datetime\n",
    "from chi import lease\n",
    "from chi import server\n",
    "from chi import context\n",
    "from chi import hardware\n",
    "\n",
    "context.version = \"1.0\" # required during transition\n",
    "context.choose_site(default=\"CHI@UC\")\n",
    "context.choose_project()\n",
    "username = os.getenv('USER') # all exp resources will have this prefix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to change the details of the Chameleon server, e.g. use a different GPU type depending on availability, you can do that in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type = \"gpu_rtx_6000\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reservation\n",
    "\n",
    "The following cells will create a reservation that begins now, and ends in 8 hours, *if* your requested node type is available.\n",
    "\n",
    "If the node type you have requested is *not* available right now, it will start your reservation as soon as one is available.\n",
    "\n",
    "You can refer to the [CHI@UC host calendar](https://chi.uc.chameleoncloud.org/project/leases/calendar/host/) to see availability (change the \"Node type\" selection to `gpu_rtx_6000`), or refer to the [CHI@TACC host calendar](https://chi.tacc.chameleoncloud.org/project/leases/calendar/host/) to see availability for other node types with GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_start_times = [n.next_free_timeslot()[0] for n in hardware.get_nodes(node_type=node_type)]\n",
    "current_time = datetime.datetime.now(datetime.timezone.utc) + datetime.timedelta(minutes=1)\n",
    "if min(gpu_start_times) > current_time:\n",
    "    lease_start = min(gpu_start_times)\n",
    "    print(f\"There is no {node_type} available now, you can request one starting at {str(lease_start)} (UTC).\")\n",
    "else:\n",
    "    lease_start = current_time\n",
    "    print(f\"A {node_type} IS available now, your lease will start right away.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = lease.Lease(f\"colab-{username}-{node_type}\", duration=datetime.timedelta(hours=8),\n",
    "                start_date = lease_start  )\n",
    "l.add_node_reservation(node_type = node_type, amount = 1) \n",
    "l.add_fip_reservation(1) \n",
    "l.submit(idempotent=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provisioning resources\n",
    "\n",
    "This section provisions resources. It will take approximately 10 minutes. You can check on its status in the Chameleon web-based UI: [https://chi.uc.chameleoncloud.org/project/instances/](https://chi.uc.chameleoncloud.org/project/instances/), then come back here when it is in the READY state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue here, whether using a lease created just now or one created earlier\n",
    "l = lease.get_lease(f\"colab-{username}-{node_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = server.Server(\n",
    "    f\"colab-{username}-{node_type}\", \n",
    "    reservation_id=l.node_reservations[0][\"id\"],\n",
    "    image_name=\"CC-Ubuntu22.04\"\n",
    ")\n",
    "s.submit(idempotent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associate an IP address with this server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reserved_fip = l.get_reserved_floating_ips()[0]\n",
    "s.associate_floating_ip(reserved_fip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and wait for it to come up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.check_connectivity(host=reserved_fip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cells will install some basic packages in order to connect your Colab frontend to your Chameleon server. However, you may want to log in to your Chameleon server in order to access its terminal and install or configure packages outside of Colab.\n",
    "\n",
    "To log in to the resource, use File > New > Terminal in the Chameleon JupyterHub environment, or your local terminal, and run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ssh cc@\" + reserved_fip)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Meanwhile, install an updated GPU driver on your resource:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chi import ssh\n",
    "\n",
    "node = ssh.Remote(reserved_fip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.run('wget https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64/cuda-keyring_1.0-1_all.deb')\n",
    "node.run('sudo dpkg -i cuda-keyring_1.0-1_all.deb')\n",
    "node.run('sudo apt update')\n",
    "node.run('sudo apt -y install linux-headers-$(uname -r)')\n",
    "node.run('sudo apt -y install nvidia-driver-560') \n",
    "node.run('sudo apt -y install cuda-12-2 cuda-runtime-12-2 cuda-drivers=560.35.03-1')\n",
    "node.run('sudo apt -y install nvidia-gds-12-2') \n",
    "node.run('sudo apt -y install libcudnn8=8.9.7.29-1+cuda12.2 cuda-toolkit-12-2') # make sure the get cuda-12-2 version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.run(\"echo 'PATH=\\\"/usr/local/cuda-12.2/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin\\\"' | sudo tee /etc/environment\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to reboot, and make sure we have the specified CUDA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    node.run('sudo reboot') # reboot and wait for it to come up\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.check_connectivity(host=reserved_fip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = ssh.Remote(reserved_fip) # note: need a new SSH session to get new PATH\n",
    "node.run('nvidia-smi')\n",
    "node.run('nvcc --version')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Docker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use common deep learning frameworks like Tensorflow or PyTorch, we can run *containers* that have all the prerequisite libraries necessary for these frameworks. Here, we will set up the container framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.run(\"curl -sSL https://get.docker.com/ | sudo sh\")\n",
    "node.run(\"sudo groupadd -f docker; sudo usermod -aG docker $USER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node = ssh.Remote(reserved_fip) # note: need a new SSH session to get new group membership\n",
    "node.run(\"docker run hello-world\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get NVIDIA container toolkit \n",
    "node.run(\"curl -fsSL https://nvidia.github.io/libnvidia-container/gpgkey | sudo gpg --dearmor -o /usr/share/keyrings/nvidia-container-toolkit-keyring.gpg \\\n",
    "  && curl -s -L https://nvidia.github.io/libnvidia-container/stable/deb/nvidia-container-toolkit.list | \\\n",
    "    sed 's#deb https://#deb [signed-by=/usr/share/keyrings/nvidia-container-toolkit-keyring.gpg] https://#g' | \\\n",
    "    sudo tee /etc/apt/sources.list.d/nvidia-container-toolkit.list\")\n",
    "node.run(\"sudo apt update\")\n",
    "node.run(\"sudo apt-get install -y nvidia-container-toolkit\")\n",
    "node.run(\"nvidia-ctk runtime configure --runtime=docker\")\n",
    "node.run(\"sudo systemctl restart docker\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check that we can see GPU from inside container\n",
    "node.run(\"docker run --rm --gpus all ubuntu nvidia-smi\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a Jupyter server"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can start a Jupyter server with your preferred deep learning framework."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Tensorflow, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.run(\"docker run -d --rm -p 8888:8888 --gpus all quay.io/jupyter/tensorflow-notebook:cuda-tensorflow-2.16.1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, after a minute or two, check the logs of this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.run(\"docker ps -q | xargs -L 1 docker logs\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output of this command, look for a line like\n",
    "\n",
    "```\n",
    "        http://127.0.0.1:8888/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "```\n",
    "\n",
    "You will copy and paste this URL into your own browser, but replace the **127.0.0.1** with the floating IP assigned to your server. This will open a Jupyter instance running *on your GPU server*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you start a notebook inside this Jupyter server, you should be able to run\n",
    "\n",
    "```\n",
    "import tensorflow as tf\n",
    "print(tf.config.list_physical_devices(\"GPU\"))\n",
    "```\n",
    "\n",
    "and see a GPU device listed."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stop the running container(s), use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.run(\"docker ps -q | xargs -L 1 docker container stop\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For PyTorch, run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node.run(\"docker run -d -p 8888:8888 --rm --gpus all quay.io/jupyter/pytorch-notebook:cuda12-latest\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, after a minute or two, check the logs of this process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.run(\"docker ps -q | xargs -L 1 docker logs\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output of this command, look for a line like\n",
    "\n",
    "```\n",
    "        http://127.0.0.1:8888/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "```\n",
    "\n",
    "You will copy and paste this URL into your own browser, but replace the **127.0.0.1** with the floating IP assigned to your server. This will open a Jupyter instance running *on your GPU server*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you start a notebook inside this Jupyter server, you should be able to run\n",
    "\n",
    "```\n",
    "import torch\n",
    "print(torch.cuda.get_device_name(0))\n",
    "```\n",
    "\n",
    "and see the name of your GPU."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stop the running container(s), use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.run(\"docker ps -q | xargs -L 1 docker container stop\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect a Colab frontend to your Jupyter server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ssh -L 127.0.0.1:8888:127.0.0.1:8888 cc@' + reserved_fip) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a **local terminal on your own laptop**, run the SSH command that is printed by the previous cell. This will set up a tunnel to the Jupyter server that you are running on a Chameleon instance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your Chameleon key is not in the default location, you should also specify the path to your key as an argument, using `-i`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ssh -i ~/.ssh/id_rsa_chameleon -L 127.0.0.1:8888:127.0.0.1:8888 cc@' + reserved_fip) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave this SSH session open."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, start your preferred container (un-comment one option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#node.run(\"docker run -d --rm -p 8888:8888 --gpus all quay.io/jupyter/tensorflow-notebook:cuda-tensorflow-2.16.1\")\n",
    "#node.run(\"docker run -d -p 8888:8888 --rm --gpus all quay.io/jupyter/pytorch-notebook:cuda12-latest\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and check the logs -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node.run(\"docker ps -q | xargs -L 1 docker logs\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for a URL in this format:\n",
    "    \n",
    "```\n",
    "http://127.0.0.1:8888/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy this URL - you will need it in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can open Colab in a browser. Click on the drop-down menu for \"Connect\" in the top right and select \"Connect to a local runtime\". Paste the URL you copied earlier into the space and click \"Connect\". Your notebook should now be running on your Colab host (you can put `!hostname` in a cell and run it to verify!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Release resources\n",
    "\n",
    "If you finish with your experimentation before your lease expires,release your resources and tear down your environment by running the following (commented out to prevent accidental deletions).\n",
    "\n",
    "This section is designed to work as a \"standalone\" portion - you can come back to this notebook, ignore the top part, and just run this section to delete your reasources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup environment - if you made any changes in the top part, make the same changes here\n",
    "import chi, os\n",
    "from chi import lease, magic, context\n",
    "\n",
    "context.version = \"1.0\" # required during transition\n",
    "\n",
    "context.choose_site(default=\"CHI@UC\")\n",
    "context.choose_project()\n",
    "username = os.getenv('USER') # all exp resources will have this prefix\n",
    "\n",
    "node_type = \"gpu_rtx_6000\"\n",
    "lease = lease.get_lease(f\"colab-{username}-{node_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# un-comment to free resources\n",
    "# chi.magic.cleanup_resources(lease.id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
