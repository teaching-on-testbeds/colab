{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connect Colab to a Chameleon server - with AMD GPU\n",
    "\n",
    "This notebook describes how to connect Colab to a server running on Chameleon. This allows you to run experiments requiring bare metal access, storage, memory, GPU and compute that exceeds the abilities of Colab's hosted runtime, but with Colab's familiar interface (and notebooks stored in your Google Drive). It also allows you to easily go back and forth between the convenience of Colab's hosted runtime and Chameleon's greater capabilities, depending on the needs of your experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Provision the resource\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check resource availability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will try to reserve a bare metal Ubuntu server with MI100 GPU on CHI@TACC - pending availability. Before you begin, you should check the host calendar at [https://chi.tacc.chameleoncloud.org/project/leases/calendar/host/](https://chi.tacc.chameleoncloud.org/project/leases/calendar/host/). In the \"Node Type\" dropdown, filter on `gpu_mi100` and make sure some hosts are available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chameleon configuration\n",
    "\n",
    "You can change your Chameleon project name (if not using the one that is automatically configured in the JupyterHub environment) and the site on which to reserve resources (depending on availability) in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chi, os, time, datetime\n",
    "from chi import lease\n",
    "from chi import server\n",
    "from chi import context\n",
    "from chi import hardware\n",
    "\n",
    "context.version = \"1.0\" # required during transition\n",
    "context.choose_site(default=\"CHI@TACC\")\n",
    "context.choose_project()\n",
    "username = os.getenv('USER') # all exp resources will have this prefix"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you need to change the details of the Chameleon server, e.g. use a different GPU type depending on availability, you can do that in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_type = \"gpu_mi100\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reservation\n",
    "\n",
    "The following cells will create a reservation that begins now, and ends in 8 hours, *if* your requested node type is available.\n",
    "\n",
    "If the node type you have requested is *not* available right now, it will start your reservation as soon as one is available.\n",
    "\n",
    "You can refer to [CHI@TACC host calendar](https://chi.tacc.chameleoncloud.org/project/leases/calendar/host/) to see availability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_start_times = [n.next_free_timeslot()[0] for n in hardware.get_nodes(node_type=node_type)]\n",
    "current_time = datetime.datetime.now(datetime.timezone.utc) + datetime.timedelta(minutes=1)\n",
    "if min(gpu_start_times) > current_time:\n",
    "    lease_start = min(gpu_start_times)\n",
    "    print(f\"There is no {node_type} available now, you can request one starting at {str(lease_start)} (UTC).\")\n",
    "else:\n",
    "    lease_start = current_time\n",
    "    print(f\"A {node_type} IS available now, your lease will start right away.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = lease.Lease(f\"colab-{username}-{node_type}\", duration=datetime.timedelta(hours=8),\n",
    "                start_date = lease_start  )\n",
    "l.add_node_reservation(node_type = node_type, amount = 1) \n",
    "l.add_fip_reservation(1) \n",
    "l.submit(idempotent=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provisioning resources\n",
    "\n",
    "This section provisions resources. It will take approximately 10 minutes. You can check on its status in the Chameleon web-based UI: [https://chi.tacc.chameleoncloud.org/project/instances/](https://chi.tacc.chameleoncloud.org/project/instances/), then come back here when it is in the READY state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continue here, whether using a lease created just now or one created earlier\n",
    "l = lease.get_lease(f\"colab-{username}-{node_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = server.Server(\n",
    "    f\"colab-{username}-{node_type}\", \n",
    "    reservation_id=l.node_reservations[0][\"id\"],\n",
    "    image_name=\"CC-Ubuntu24.04-hwe\"  # warning! default Ubuntu 24.04 kernel is not compatible with MI100\n",
    ")\n",
    "s.submit(idempotent=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Associate an IP address with this server:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.associate_floating_ip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and wait for it to come up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.refresh()\n",
    "reserved_fip = s.get_floating_ip()\n",
    "s.check_connectivity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log in to resource"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To log in to the resource, use File > New > Terminal in the Chameleon JupyterHub environment, or your local terminal, and paste in the *output* of the following cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ssh cc@\" + reserved_fip)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up AMD driver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use the AMD GPUs, we'll set up the driver using the `amdgpu-install` utility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.execute(\"sudo apt update; wget https://repo.radeon.com/amdgpu-install/6.3.3/ubuntu/noble/amdgpu-install_6.3.60303-1_all.deb\")\n",
    "s.execute(\"sudo apt -y install ./amdgpu-install_6.3.60303-1_all.deb; sudo apt update\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.execute(\"amdgpu-install -y --usecase=dkms\")\n",
    "s.execute(\"sudo apt -y install rocm-smi \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.execute(\"sudo usermod -a -G video,render cc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point you need to reboot:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.execute(\"sudo reboot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and wait for the host to come back online:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.check_connectivity()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it does, you should be able to see the GPU(s):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.execute(\"rocm-smi\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Docker"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use common deep learning frameworks like Tensorflow or PyTorch, we can run *containers* that have all the prerequisite libraries necessary for these frameworks. Here, we will set up the container framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.execute(\"curl -sSL https://get.docker.com/ | sudo sh\")\n",
    "s.execute(\"sudo groupadd -f docker; sudo usermod -aG docker $USER\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.execute(\"docker run hello-world\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Docker container with ROCm and PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROCm (Radeon Open Compute Platform) is an open-source software stack from AMD that allows users to program AMD GPUs (similar to NVIDIA's CUDA). To use our AMD GPU for machine learning, we'll install ROCm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will build a container image with ROCm and PyTorch, so that we can run deep learning jobs on this server with PyTorch. You can follow a similar approach to build a container image with Tensorflow or similar frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.execute(\"wget https://raw.githubusercontent.com/teaching-on-testbeds/colab/refs/heads/main/docker/Dockerfile.pytorch-notebook-rocm\")\n",
    "s.execute(\"docker build -t pytorch-notebook-rocm -f Dockerfile.pytorch-notebook-rocm .\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start a Jupyter server"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can start a Jupyter server with Pytorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: the extra group is needed because https://github.com/ROCm/ROCm-docker/issues/90\n",
    "s.execute(\"docker run  -d --rm  -p 8888:8888 --device=/dev/kfd --device=/dev/dri --group-add video --group-add $(getent group | grep render | cut -d':' -f 3) pytorch-notebook-rocm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.execute(\"docker ps -q | xargs -L 1 docker logs\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the output of this command, look for a line like\n",
    "\n",
    "```\n",
    "        http://127.0.0.1:8888/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "```\n",
    "\n",
    "You will copy and paste this URL into your own browser, but replace the **127.0.0.1** with the floating IP assigned to your server. This will open a Jupyter instance running *on your GPU server*."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you start a notebook inside this Jupyter server, you should be able to run\n",
    "\n",
    "```\n",
    "import torch\n",
    "print(torch.cuda.get_device_name(0))\n",
    "```\n",
    "\n",
    "and see the name of your GPU. \n",
    "\n",
    "You should also be able to run \n",
    "\n",
    "```\n",
    "!rocminfo\n",
    "```\n",
    "\n",
    "and see details of the GPU(s)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To stop the running container(s), use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.execute(\"docker ps -q | xargs -L 1 docker container stop\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect a Colab frontend to your Jupyter server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ssh -L 127.0.0.1:8888:127.0.0.1:8888 cc@' + reserved_fip) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a **local terminal on your own laptop**, run the SSH command that is printed by the previous cell. This will set up a tunnel to the Jupyter server that you are running on a Chameleon instance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your Chameleon key is not in the default location, you should also specify the path to your key as an argument, using `-i`. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('ssh -i ~/.ssh/id_rsa_chameleon -L 127.0.0.1:8888:127.0.0.1:8888 cc@' + reserved_fip) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Leave this SSH session open."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, start your preferred container (un-comment one option)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.execute(\"docker run  -d --rm  -p 8888:8888 --device=/dev/kfd --device=/dev/dri --group-add video --group-add $(getent group | grep render | cut -d':' -f 3) pytorch-notebook-rocm\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and check the logs -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s.execute(\"docker ps -q | xargs -L 1 docker logs\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look for a URL in this format:\n",
    "    \n",
    "```\n",
    "http://127.0.0.1:8888/?token=XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy this URL - you will need it in the next step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can open Colab in a browser. Click on the drop-down menu for \"Connect\" in the top right and select \"Connect to a local runtime\". Paste the URL you copied earlier into the space and click \"Connect\". Your notebook should now be running on your Colab host (you can put `!hostname` in a cell and run it to verify!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Release resources\n",
    "\n",
    "If you finish with your experimentation before your lease expires,release your resources and tear down your environment by running the following (commented out to prevent accidental deletions).\n",
    "\n",
    "This section is designed to work as a \"standalone\" portion - you can come back to this notebook, ignore the top part, and just run this section to delete your reasources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup environment - if you made any changes in the top part, make the same changes here\n",
    "import chi, os\n",
    "from chi import lease, magic, context\n",
    "\n",
    "context.version = \"1.0\" # required during transition\n",
    "\n",
    "context.choose_site(default=\"CHI@UC\")\n",
    "context.choose_project()\n",
    "username = os.getenv('USER') # all exp resources will have this prefix\n",
    "\n",
    "node_type = \"gpu_mi100\"\n",
    "l = lease.get_lease(f\"colab-{username}-{node_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# un-comment to free resources\n",
    "# chi.magic.cleanup_resources(l.id)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
